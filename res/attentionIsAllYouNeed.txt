切分结果:
<切分结果起始>
标题: Transformer模型概述
摘要: 本文提出了一种新的基于注意力机制的神经网络架构——Transformer，该模型完全摒弃了传统的循环神经网络和卷积神经网络结构。
内容: The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring signiﬁcantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-
to-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.0 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature. We show that the Transformer generalizes well to
other tasks by applying it successfully to English constituency parsing both with
large and limited training data.

$$

标题: Transformer模型的引言
摘要: 本部分介绍了循环神经网络在序列建模和机器翻译中的应用，并指出了其在长序列处理中的局限性。
内容: 1 Introduction
Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks
in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and
transduction problems such as language modeling and machine translation [ 33,2,5]. Numerous
efforts have since continued to push the boundaries of recurrent language models and encoder-decoder
architectures [36, 23, 15].
Recurrent models typically factor computation along the symbol positions of the input and output
sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden
statesht, as a function of the previous hidden state ht−1and the input for position t. This inherently
sequential nature precludes parallelization within training examples, which becomes critical at longer
sequence lengths, as memory constraints limit batching across examples. Recent work has achieved
signiﬁcant improvements in computational efﬁciency through factorization tricks [ 20] and conditional
computation [ 31], while also improving model performance in case of the latter. The fundamental
constraint of sequential computation, however, remains.

$$

标题: Transformer模型架构
摘要: 本部分详细介绍了Transformer模型的架构，包括编码器和解码器的堆叠层结构，以及多头注意力和逐位置全连接层的实现。
内容: 3 Model Architecture
Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,33].
Here, the encoder maps an input sequence of symbol representations (x1,...,x n)to a sequence
of continuous representations z= (z1,...,z n). Given z, the decoder then generates an output
sequence (y1,...,y m)of symbols one element at a time. At each step the model is auto-regressive
[10], consuming the previously generated symbols as additional input when generating the next.
The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.
3.1 Encoder and Decoder Stacks
Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two
sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-
wise fully connected feed-forward network. We employ a residual connection [ 11] around each of
the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is
LayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer
itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding
layers, produce outputs of dimension dmodel = 512 .

$$

标题: Transformer模型的注意力机制
摘要: 本部分深入讨论了Transformer模型中使用的注意力机制，包括缩放点积注意力和多头注意力的实现方式。
内容: 3.2 Attention
An attention function can be described as mapping a query and a set of key-value pairs to an output,
where the query, keys, values, and output are all vectors. The output is computed as a weighted sum
of the values, where the weight assigned to each value is computed by a compatibility function of the
query with the corresponding key.
3.2.1 Scaled Dot-Product Attention
We call our particular attention "Scaled Dot-Product Attention" (Figure 2). The input consists of
queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the
query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the
values.

$$

标题: Transformer模型的位置编码
摘要: 本部分解释了Transformer模型中如何通过位置编码来使模型感知序列中单词的顺序。
内容: 3.5 Positional Encoding
Since our model contains no recurrence and no convolution, in order for the model to make use of the
order of the sequence, we must inject some information about the relative or absolute position of the
tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the
bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel
as the embeddings, so that the two can be summed.

$$

标题: Transformer模型的训练细节
摘要: 本部分描述了Transformer模型的训练过程，包括训练数据、批处理、硬件配置、优化器选择和正则化策略。
内容: 5 Training
This section describes the training regime for our models.
5.1 Training Data and Batching
We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million
sentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-
target vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT
2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece
vocabulary [ 36]. Sentence pairs were batched together by approximate sequence length. Each training
batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000
target tokens.

$$

标题: Transformer模型的实验结果
摘要: 本部分展示了Transformer模型在机器翻译和英语成分句法分析任务上的性能，并与其他模型进行了比较。
内容: 6 Results
6.1 Machine Translation
On the WMT 2014 English-to-German translation task, Our big transformer model (Transformer
(big) in Table 2) outperforms the best previously reported models (including ensembles) by more
than2.0BLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this
model is listed in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base
model surpasses all previously published models and ensembles, at a fraction of the training cost of
any of the previous best models.

$$

标题: Transformer模型的结论和未来工作
摘要: 本部分总结了Transformer模型的主要贡献，并提出了未来的研究方向。
内容: 7 Conclusion
In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on
attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with
multi-headed self-attention.

$$

标题: 对Transformer模型注意力分布的可视化分析
摘要: 本部分通过可视化的方式展示了Transformer模型在处理长距离依赖和句子结构时注意力的分布情况。
内容: 12Attention Visualizations
Input-Input Layer5
It
is
in
this
spirit
that
a
majority
of
American
governments
have
passed
new
laws
since
2009
making
the
registration
or
voting
process
more
difficult
.
<EOS>
<pad>
<pad>
<pad>
<pad>
<pad>
<pad>
<pad>
It
is
in
this
spirit
that
a
majority
of
American
governments
have
passed
new
laws
since
2009
making
the
registration
or
voting
process
more
difficult
.
<EOS>
<pad>
<pad>
<pad>
<pad>
<pad>
<pad>
<pad>
Figure 3: An example of the attention mechanism following long-distance dependencies in the
encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of
the verb ‘making’, completing the phrase ‘making...more difﬁcult’. Attentions here shown only for
the word ‘making’. Different colors represent different heads. Best viewed in color.

$$

<切分结果结束>